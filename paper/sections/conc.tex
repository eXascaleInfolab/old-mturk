%!TEX root = ../dynamics.tex
\section{Conclusions}\label{sec:conc}
We studied data collected from a popular micro-task crowdsourcing platform, \amt{},
and analyzed a number of key dimensions of the platform, including: topic, task type, reward evolution, platform throughput, and supply and demand. The results of our analysis can serve as a starting point for improving existing crowdsourcing platforms and for optimizing the overall efficiency and effectiveness of human computation systems. The evidence presented above indicate how requesters should use crowdsourcing platforms to obtain the best out of them: By engaging with workers and publishing large volumes of HITs at specific points in time.

Future research based on this work might look at different directions. On one hand, novel micro-task crowdsourcing platforms need to be designed based on the findings identified in this work, such as the need for supporting specific task types like audio transcription or surveys. 
Additionally, analyses that look at specific data could provide a deeper understanding of the micro-task crowdsourcing universe. Examples include per-requester or per-task analyses of the publishing behavior rather than looking at the entire market evolution as we did in this work.
Similarly, a worker-centered analysis could provide additional evidence of the existence of different classes of workers, e.g., full-time vs casual workers, or workers specializing on specific task types as compared to generalists who are willing to complete any available task.
While a requester-centered analysis would consider information about the requesters' reputation, pricing and HIT types.

\section{Acknowledgments}
We thank the reviewers for their helpful comments.
%
This work was supported by the Swiss National Science Foundation under grant number PP00P2\_128459.