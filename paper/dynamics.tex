\documentclass{sig-alternate}
\usepackage{graphicx}
\usepackage{url}
\usepackage[usenames,dvipsnames]{color}

\newcommand{\gd}[1]{\textcolor{ForestGreen}{GD: #1}}

\begin{document}
%
% --\item Author Metadata here ---
\conferenceinfo{WWW}{'15}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden \item IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden \item IF NEED BE.
% --\item End of Author Metadata ---

% \title{The Evolution of Micro-task Crowdsourcing Markets}
\title{The Dynamics of Micro-Task Crowdsourcing}
\subtitle{The Case of Amazon MTurk}
% The Evolution of Micro-Task Crowdsourcing Markets â€”- The Case of Amazon Mechanical Turk
% The Dynamics of Micro-Task Crowdsourcing -- The Case of Amazon MTurk
% Anatomy of a Micro-Task Crowdsourcing Platform
% Unravelling Micro-Task Crowdsourcing Dynamics/Processes
% The Market for HITs: Throughput Uncertainty and the Market Mechanism


%\numberofauthors{1}
\author{Djellel E. Difallah, Michele Catasta, Gianluca Demartini, \\ Panagiotis G. Ipeirotis, Philippe Cudr\'e-Mauroux}

\maketitle
\begin{abstract}
Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other ``technology'', a crowdsourcing platform is subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the \emph{dynamics} of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a task would lead to faster results. However, it is still unclear how different dimensions interact each other: reward, task type, market competition, requester reputation, etc.

In this paper we adopt a data-driven approach to perform a long-term analysis of a popular micro-task crowdsourcing platform to understand behavior evolution of its main actors: workers, requesters, tasks, and platform. We leverage the main findings of our five year log analysis to generate features used in a predictive model for the expected performance of a batch of tasks published on the crowdsourcing platform at a specific point in time by a certain requester.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Design, Experimentation, Human Factors}

\keywords{Crowdsourcing, social networks, log analysis}

\input{sections/intro}
\input{sections/relwork}

\section{The Amazon Mechanical Turk Crowdsourcing Platform}

\input{sections/stats}
\input{sections/hittype}
\input{sections/throughput}
\input{sections/market}

\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{crowd}

\end{document}
