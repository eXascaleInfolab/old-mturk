\documentclass{sig-alternate}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}

\newcommand{\pcm}[1]{\textcolor{magenta}{PCM: #1}}
\newcommand{\gd}[1]{\textcolor{ForestGreen}{GD: #1}}

\begin{document}
%
% --\item Author Metadata here ---
\conferenceinfo{WWW}{'15}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden \item IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden \item IF NEED BE.
% --\item End of Author Metadata ---

\title{The Evolution of Micro-task Crowdsourcing Markets}
\subtitle{The Case of Amazon Mechanical Turk}
% \title{The Dynamics of Micro-task Crowdsourcing -- The Case of MTurk}
% anatomy of a Micro-task Crowdsourcing platform
% unravelling micro-task crowdsourcing dynamics/processes
% the evolution of micro-task crowdsourcing



%\numberofauthors{1}
\author{}

\maketitle
\begin{abstract}

Micro-task crowdsourcing is gaining popularity among serval organizations and research corps to leverage Human Computation in their daily operations. Unlike any other ``technology'', a crowdsourcing platform is subject to many factors that affect the performance, both in terms of speed and quality, of its services. Indeed, such factors shape the \emph{dynamics} of the market. For example, a common result is that increasing the price of a HIT would lead to faster results, however, we still do not know the exact impact of changing the price in the presence of many reputable requesters on the platform. Or, what happen when you post a link of a batch on a popular crowdsourcing forum etc.

In this paper we adopt a data-driven approach to analyze the behavior of the main actors in MTurk micro-task crowdsourcing: workers, requesters, HITs, and platform (MTurk); thanks to a collection of datasets that we gathered during the last 5 year.

The ultimate contribution that we propose is a predictive model to derive the expected performance of a published batch of HITs on MTurk at a specific moment in time.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Design, Experimentation, Human Factors}

\keywords{Crowdsourcing, social networks, log analysis}

\input{sections/intro}
\input{sections/relwork}

\section{The Amazon Mechanical Turk Crowdsourcing Platform}

\section{The Evolution of MTurk}
2009-2014
\subsection{A Data-driven Analysis}
Datasets: tracker, to, 
\subsection{Why Did Certain Topics Became Popular?}
\subsection{Why Did Certain Countries Were Preferred?}
top keywords per country (over time)
\subsection{Why Did Certain Requesters Quit?}
% \subsection{Does reputation Improve Throughput?}

\section{Large-Scale HIT Type Analysis}

\subsection{HIT Types}
\begin{itemize}

	\item CA

	\item CC

	\item IA

	\item IF

	\item SU

	\item VV

\end{itemize}

\subsection{Classification of HITs}
\begin{itemize}

	\item sample of 5000 HITs with type labelled by means of crowdsourcing. We asked workers on MTurk to assign a HIT to one of the defined classes by presenting them with the title, description, keywords, reward, allotted time for the HIT. The instructions contained a definition and examples for each task type.

	\item After assigning each HIT to three different workers in the crowd, a consensus on the task type was reached in 89\% of the cases (551 cases with no clear majority).

%evaluation
	\item Using the labelled data we trained a multi-class SVM classifier for the 6 different task types and evaluated its quality with 10-folds cross validation over the labelled dataset. Overall, the classifier obtained Precision of 0.895, Recall of 0.899, and F-Measure of 0.895. Most classifier errors (i.e., 66 instances) were performed by incorrectly classifying IA instances as CC.

	\item Using a classifier trained over the entire labelled data, we performed a large scale classification of the 2.5M HITs in our collection. This allow us to study the evolution of task type on the market.

\end{itemize}






\input{sections/throughput}

\section{Conclusions}

\end{document}
