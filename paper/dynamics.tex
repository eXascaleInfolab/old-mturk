\documentclass{sig-alternate}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amssymb}
\usepackage[usenames,dvipsnames]{color}
\usepackage{enumitem}
\usepackage{float}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}

\newcommand{\gd}[1]{\textcolor{ForestGreen}{GD: #1}}
\newcommand{\amt}[1]{Amazon MTurk}

\begin{document}
%
% --\item Author Metadata here ---
\conferenceinfo{WWW}{'15}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden \item IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden \item IF NEED BE.
% --\item End of Author Metadata ---

% \title{The Evolution of Micro-task Crowdsourcing Markets}
\title{The Dynamics of Micro-Task Crowdsourcing}
\subtitle{The Case of Amazon MTurk}
% The Evolution of Micro-Task Crowdsourcing Markets â€”- The Case of Amazon Mechanical Turk
% The Dynamics of Micro-Task Crowdsourcing -- The Case of Amazon MTurk
% Anatomy of a Micro-Task Crowdsourcing Platform
% Unravelling Micro-Task Crowdsourcing Dynamics/Processes
% The Market for HITs: Throughput Uncertainty and the Market Mechanism


\numberofauthors{1}
\author{
\alignauthor
Djellel E. Difallah$^*$, Michele Catasta$^\dagger$, Gianluca Demartini$^\ddagger$, \\  Panagiotis G. Ipeirotis$^\diamond$, Philippe Cudr\'e-Mauroux$^*$\\
       \affaddr{$^*$ eXascale Infolab, University of Fribourg, Switzerland}\\
       \affaddr{$^\dagger$ EFPL, Switzerland}\\
       \affaddr{$^\ddagger$ University of Sheffield, UK}\\
	   \affaddr{$^\diamond$ New York University, USA}
       % \email{trovato@corporation.com}
}

\maketitle
\begin{abstract}
Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the \emph{dynamics} of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc.

In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, tasks, and platform). (B) We leverage the main findings of our five year log analysis to propose features used in a predictive model aiming at determining the expected performance of any batch at a specific point in time. We show that the number of tasks left in a batch and the time at which the batch was posted are two key features of the prediction. (C) Finally, we conduct an analysis of the demand (new tasks posted by the requesters) and supply (number of tasks completed by the workforce) and show how they affect task prices on the marketplace.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Design, Experimentation, Human Factors}

\keywords{Crowdsourcing, trend identification, tracking, forecasting}

\input{sections/intro}
\input{sections/relwork}



\input{sections/stats}
\input{sections/hittype}
\input{sections/throughput}
\input{sections/market}

\input{sections/discussion}
\input{sections/conc}

\input{sections/appendix.tex}

\bigskip
%\small
\bibliographystyle{abbrv}
\bibliography{crowd}



\end{document}

